{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 DL Studio\n",
    "### Run CIFAR-10 Example\n",
    "For Net, the overall accuracy of the network on the 10000 test images: 53 %\n",
    "For Net2, the overall accuracy of the network on the 10000 test images: 67 %\n",
    "\n",
    "Net2 outperforms Net, the primary reason for this improvement is that Net2 has an additional convolutional layer and features more channels in each layer, enhancing its ability to learn more complex patterns.\n",
    "\n",
    "### Analyze Resolution Changes\n",
    "CIFAR-10 images are typically of size 32x32 pixels and have 3 color channels (RGB), which is (batch_size, 3, 32, 32).\n",
    "\n",
    "Let's use Net for analyzing resolution:\n",
    "```self.conv1 = nn.Conv2d(3, 6, 5) (input channel, output channel, kernel size)```\n",
    "Output -> The kernel size is 5x5 with a stride of 1 (default), so the spatial dimensions (height and width) of the image are reduced by 4 pixels. Hence, the output will be (batch_size, 6, 28, 28) (32-5+1)\n",
    "```x = nn.MaxPool2d(2, 2)(F.relu(self.conv1(x)))```\n",
    "Max pooling with a 2x2 kernel halves the spatial resolution. Output will be (batch_size, 6, 14, 14)\n",
    "```self.conv2 = nn.Conv2d(6, 16, 5)```\n",
    "Output: (batch_size, 16, 10, 10)\n",
    "```x = nn.MaxPool2d(2, 2)(F.relu(self.conv2(x)))```\n",
    "Output: (batch_size, 16, 5, 5)\n",
    "```x = x.view(x.shape[0], -1)```\n",
    "Output: (batch_size, 16*5*5 = 400)\n",
    "\n",
    "And FC layers, which is easy, for example, fc1:\n",
    "```self.fc1 = nn.Linear(16 * 5 * 5, 120)```\n",
    "Output: (batch_size, 120)\n",
    "\n",
    "### Consider Kernel Sizes and Padding\n",
    "For Net, using a 5x5 kernel in Net is relatively large. A larger kernel can be beneficial for reducing the number of layers and capturing bigger features. However, it also results in more parameters, which increases the risk of overfitting. Typically, architectures like VGG and ResNet use 3x3 kernels, which are more efficient for deeper neural networks. We can also use a hybrid approach by combining different kernel sizes to capture multi-scale information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Dataset Creation Instructions\n",
    "To create a personal dataset by extracting a subset of the COCO dataset, we can utilize the provided code in the manual. However, to store the images separately for the training and validation sets, we introduce an additional list, images_to_process, which will temporarily store the image metadata.\n",
    "\n",
    "Instead of saving the images directly in the for loop, we append the image metadata to this list. Afterward, we can split the images_to_process list into two separate lists—one for the training images and the other for validation images. Finally, we use two separate for loops to save the images accordingly into their respective directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 5x3 Images\n",
    "\n",
    "##### Single instance\n",
    "##### Multi instance (single object)\n",
    "##### Multi instance (diff object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 3x5 Tables\n",
    "\n",
    "##### Train\n",
    "##### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Image Classification using CNNs– Training and Validation\n",
    "### CNN for Dataset 1\n",
    "\n",
    "##### Train loss plot\n",
    "##### Confusion Matrix plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN for Dataset 2\n",
    "\n",
    "##### Train loss plot\n",
    "##### Confusion Matrix plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN for Dataset 3\n",
    "\n",
    "##### Train loss plot\n",
    "##### Confusion Matrix plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from Conf matrix and table of accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot misclassified images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "### Tain/confmatrices\n",
    "### Observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Set COCO dataset paths\n",
    "data_dir = os.getcwd()\n",
    "ann_file = os.path.join(data_dir, \"annotations/instances_train2014.json\")  # Use 2014 annotations\n",
    "image_dir = os.path.join(data_dir, \"train2014/train2014\")  # Ensure correct image folder\n",
    "output_dir = \"output_datasets\"\n",
    "\n",
    "# Load COCO dataset\n",
    "coco = COCO(ann_file)\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def save_image(img_info, category_name, dataset_type, split):\n",
    "    \"\"\"Saves the extracted image into a structured output directory for training/validation.\"\"\"\n",
    "    img_path = os.path.join(image_dir, img_info['file_name'])\n",
    "    save_dir = os.path.join(output_dir, dataset_type, split, category_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = Image.open(img_path).resize((64, 64))\n",
    "    img.save(os.path.join(save_dir, img_info['file_name']))\n",
    "\n",
    "def extract_images(cat_names, min_instances=1, max_instances=1, multiple_categories=False, dataset_type=\"single_instance\"):\n",
    "    \"\"\"\n",
    "    Extracts images based on object count conditions for single-instance or multi-instance datasets.\n",
    "    \"\"\"\n",
    "    cat_ids = coco.getCatIds(catNms=cat_names)\n",
    "    img_ids = coco.getImgIds(catIds=cat_ids)\n",
    "    extracted = 0  # Counter for extracted images\n",
    "\n",
    "    # List to store valid images\n",
    "    images_to_process = [] \n",
    "\n",
    "    for img_id in img_ids:\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id, iscrowd=False)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "        # Count objects per category\n",
    "        obj_counts = {}\n",
    "        for ann in anns:\n",
    "            obj_category = coco.loadCats(ann['category_id'])[0]['name']\n",
    "            obj_counts[obj_category] = obj_counts.get(obj_category, 0) + 1\n",
    "\n",
    "        if multiple_categories:\n",
    "            # Ensure multiple object categories are present\n",
    "            if len(obj_counts) >= 2:\n",
    "                images_to_process.append(img_info)\n",
    "                extracted += 1\n",
    "        elif dataset_type == \"multi_instance_same\":\n",
    "            # Ensure multiple instances of the same object category\n",
    "            if cat_names[0] in obj_counts and obj_counts[cat_names[0]] >= min_instances:\n",
    "                images_to_process.append(img_info)\n",
    "                extracted += 1\n",
    "        else:\n",
    "            # Ensure the number of instances falls within the desired range for single-instance\n",
    "            if all(obj in obj_counts for obj in cat_names) and min_instances <= obj_counts[cat_names[0]] <= max_instances:\n",
    "                images_to_process.append(img_info)\n",
    "                extracted += 1\n",
    "\n",
    "        if extracted >= 500:  # Limit to 500 images per dataset for testing (could be adjusted)\n",
    "            break\n",
    "\n",
    "    # Split the images into training and validation sets (400 training, 100 validation)\n",
    "    train_images = images_to_process[:400]\n",
    "    val_images = images_to_process[400:500]\n",
    "\n",
    "    # Save training and validation images\n",
    "    for img_info in train_images:\n",
    "        save_image(img_info, cat_names[0], dataset_type, 'train')\n",
    "\n",
    "    for img_info in val_images:\n",
    "        save_image(img_info, cat_names[0], dataset_type, 'val')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
