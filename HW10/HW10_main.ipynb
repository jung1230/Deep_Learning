{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir = /scratch/gilbreth/chen4126/ECE60146/HW10\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Add DLStudio-2.5.2 and AdversarialLearning to sys.path so Python can find DLStudio and AdversarialLearning\n",
    "current_dir = os.getcwd()\n",
    "print(\"current_dir = %s\" % current_dir)\n",
    "\n",
    "DLStudio_dir = os.path.join(current_dir, \"../DLStudio-2.5.3\")\n",
    "sys.path.append(DLStudio_dir)\n",
    "# Transformer_dir = os.path.join(current_dir, \"../DLStudio-2.5.3\")\n",
    "# sys.path.append(Transformer_dir)\n",
    "\n",
    "from DLStudio import *\n",
    "from Transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Size of the English vocab in the dataset:  11258\n",
      "\n",
      "Size of the Spanish vocab in the dataset:  21823\n",
      "\n",
      "\n",
      "The number of learnable parameters in the Master Encoder: 1255424\n",
      "\n",
      "The number of learnable parameters in the Master Decoder: 7065663\n",
      "\n",
      "\n",
      "\n",
      "Number of sentence pairs in the dataset:  10000\n",
      "\n",
      "No sentence is longer than 10 words (including the SOS and EOS tokens)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Maximum number of training iterations in each epoch: 200\n",
      "\n",
      "\n",
      "\n",
      "[epoch: 1/40  iter: 200  elapsed_time: 6788 secs]     loss: 1.1671\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 144\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe number of learnable parameters in the Master Decoder: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m number_of_learnable_params_in_decoder)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m masking:\n\u001b[0;32m--> 144\u001b[0m     \u001b[43mxformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_code_for_training_TransformerFG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdls\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmaster_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmaster_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdisplay_train_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mcheckpoints_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints_with_masking_FG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     xformer\u001b[38;5;241m.\u001b[39mrun_code_for_training_TransformerFG(dls,master_encoder,master_decoder,display_train_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    148\u001b[0m                                                                                         checkpoints_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints_no_masking_FG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/gilbreth/chen4126/ECE60146/HW10/../DLStudio-2.5.3/Transformers/Transformers.py:1013\u001b[0m, in \u001b[0;36mTransformerFG.run_code_for_training_TransformerFG\u001b[0;34m(self, dls, master_encoder, master_decoder, display_train_loss, checkpoints_dir)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m di \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(es_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):                                                                         \u001b[38;5;66;03m## (Q)\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(predicted_word_logprobs[:,di], es_tensor[:,di])                                        \u001b[38;5;66;03m## (R)\u001b[39;00m\n\u001b[0;32m-> 1013\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                                                                                              \u001b[38;5;66;03m## (S)\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m master_encoder_optimizer\u001b[38;5;241m.\u001b[39mstep_and_update_lr()                                                                \u001b[38;5;66;03m## (T)\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m master_decoder_optimizer\u001b[38;5;241m.\u001b[39mstep_and_update_lr()                                                                \u001b[38;5;66;03m## (U)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "##  seq2seq_with_transformerFG.py\n",
    "\n",
    "\"\"\"\n",
    "    This script is for experimenting with TransformerFG.\n",
    "\n",
    "    For an introduction to TransformerFG, read the large comment block associated\n",
    "    with the definition of this class in the Transformers co-class of DLStudio.\n",
    "\n",
    "    Also read the doc block associated with the other transformer class, TransformerPreLN,\n",
    "    for the difference between TransformerFG and TransformerPreLN.\n",
    "\n",
    "    To run this example, you will need to have installed at least one of the following\n",
    "    two English-to-Spanish translation dataset archives:\n",
    "\n",
    "          en_es_xformer_8_10000.tar.gz\n",
    "\n",
    "          en_es_xformer_8_90000.tar.gz\n",
    "\n",
    "    The first consists of 10,000 pairs of English-Spanish sentences and the second\n",
    "    90,0000 such pairs.\n",
    "\n",
    "    The maximum number of words in any sentence, English or Spanish, is 8.  When you\n",
    "    include the sentence delimiter tokens SOS and EOS, that makes for a max length of\n",
    "    10 for the sentences.\n",
    "\n",
    "\n",
    "    RECOMMENDATION:\n",
    "\n",
    "       I recommend that you first try to run this script with exactly the settings\n",
    "       that are currently in the script:\n",
    "\n",
    "              1.  Use the smaller debugging dataset for a faster turn-around from\n",
    "                  the code:\n",
    "\n",
    "                         en_es_xformer_8_10000.tar.gz\n",
    "\n",
    "              2.  Use the option\n",
    "\n",
    "                         masking = True      \n",
    "\n",
    "              3.  epochs =  40\n",
    "\n",
    "       Note that with the smaller dataset, you will only get one training iteration \n",
    "       per epoch, assuming you are using a batch-size of 50.\n",
    "\n",
    "       Subsequently, try running the script for the larger dataset.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os, sys\n",
    "\n",
    "seed = 0           \n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "numpy.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmarks=False\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "##  watch -d -n 0.5 nvidia-smi\n",
    "\n",
    "# from DLStudio import *\n",
    "# from Transformers import *\n",
    "\n",
    "dataroot = \"./../../data/DataForXformer/\"\n",
    "#dataroot = \"/home/kak/TextDatasets/en_es_corpus_xformer/\"\n",
    "#dataroot = \"/mnt/cloudNAS3/Avi/TextDatasets/en_es_corpus_xformer/\"\n",
    "\n",
    "data_archive =  \"en_es_xformer_8_10000.tar.gz\"                     ## for debugging only\n",
    "#data_archive =  \"en_es_xformer_8_90000.tar.gz\"\n",
    "\n",
    "\n",
    "max_seq_length = 10\n",
    "\n",
    "embedding_size = 256        \n",
    "#embedding_size = 128\n",
    "#embedding_size = 64        \n",
    "\n",
    "num_basic_encoders = num_basic_decoders = num_atten_heads = 4     \n",
    "#num_basic_encoders = num_basic_decoders = num_atten_heads = 2    \n",
    "\n",
    "#optimizer_params = {'beta1' : 0.9,  'beta2': 0.98,  'epsilon' : 1e-9}\n",
    "optimizer_params = {'beta1' : 0.9,  'beta2': 0.98,  'epsilon' : 1e-6}\n",
    "\n",
    "num_warmup_steps = 4000\n",
    "\n",
    "masking = True                     ## for better results\n",
    "#masking = False\n",
    "\n",
    "dls = DLStudio(\n",
    "                dataroot = dataroot,\n",
    "                path_saved_model = {\"encoder_FG\" : \"./saved_encoder_FG\", \n",
    "                                    \"decoder_FG\" : \"./saved_decoder_FG\", \n",
    "                                    \"embeddings_generator_en_FG\" : \"./saved_embeddings_generator_en_FG\",\n",
    "                                    \"embeddings_generator_es_FG\" : \"./saved_embeddings_generator_es_FG\",\n",
    "                                   },\n",
    "                batch_size = 50,\n",
    "                use_gpu = True,\n",
    "                epochs = 40,\n",
    "              )\n",
    "\n",
    "xformer = TransformerFG( \n",
    "                        dl_studio = dls,\n",
    "                        dataroot = dataroot,\n",
    "                        data_archive = data_archive,\n",
    "                        max_seq_length = max_seq_length,\n",
    "                        embedding_size = embedding_size,\n",
    "                        save_checkpoints = True,\n",
    "                        num_warmup_steps = num_warmup_steps,\n",
    "                        optimizer_params = optimizer_params,\n",
    "          )\n",
    "\n",
    "master_encoder = TransformerFG.MasterEncoder(\n",
    "                                  dls,\n",
    "                                  xformer,\n",
    "                                  num_basic_encoders = num_basic_encoders,\n",
    "                                  num_atten_heads = num_atten_heads,\n",
    "                 )    \n",
    "\n",
    "\n",
    "master_decoder = TransformerFG.MasterDecoderWithMasking(\n",
    "                                  dls,\n",
    "                                  xformer, \n",
    "                                  num_basic_decoders = num_basic_decoders,\n",
    "                                  num_atten_heads = num_atten_heads,\n",
    "                                  masking = masking\n",
    "                 )\n",
    "\n",
    "\n",
    "number_of_learnable_params_in_encoder = sum(p.numel() for p in master_encoder.parameters() if p.requires_grad)\n",
    "print(\"\\n\\nThe number of learnable parameters in the Master Encoder: %d\" % number_of_learnable_params_in_encoder)\n",
    "\n",
    "number_of_learnable_params_in_decoder = sum(p.numel() for p in master_decoder.parameters() if p.requires_grad)\n",
    "print(\"\\nThe number of learnable parameters in the Master Decoder: %d\" % number_of_learnable_params_in_decoder)\n",
    "\n",
    "if masking:\n",
    "    xformer.run_code_for_training_TransformerFG(dls,master_encoder,master_decoder,display_train_loss=True,\n",
    "                                                                                     checkpoints_dir=\"checkpoints_with_masking_FG\")\n",
    "else:\n",
    "    xformer.run_code_for_training_TransformerFG(dls,master_encoder,master_decoder,display_train_loss=True,\n",
    "                                                                                        checkpoints_dir=\"checkpoints_no_masking_FG\")\n",
    "\n",
    "#import pymsgbox\n",
    "#response = pymsgbox.confirm(\"Finished training.  Start evaluation?\")\n",
    "\n",
    "#if response == \"OK\": \n",
    "xformer.run_code_for_evaluating_TransformerFG(master_encoder, master_decoder, 'myoutput.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 FG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PreLN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 5 Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 FG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 PreLN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Levenshtein metrics 2x5 table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 FG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 PreLN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
